<!doctype html>
<html lang="en">
<head>
<title>Images Are All You Need: A Look at ImageBrush</title>
<meta property="og:title" content="Images Are All You Need: A Look at ImageBrush" />
<meta name="twitter:title" content="Images Are All You Need: A Look at ImageBrush" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Images with Text</title>
  <style>
    /* Optional: Add some styling to improve the appearance */
    body {
      font-family: Arial, sans-serif;
    }

    .image-container {
      display: flex;
      align-items: center;
      margin-bottom: 20px;
    }

    .image-container img {
      width: 100px; /* Adjust the width as needed */
      margin-right: 10px;
    }
  </style>

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Images Are All You Need: A Look at ImageBrush</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation'</h2>
    <p> As the title of the paper implies, ImageBrush is a novel diffusion-based image genertation framework.
        For modern image generation networks it is common to utilize text input to with, like with a CLIP model, to
        guide the generation of an image. This approach however comes with the inherent issue of a modality gap, where
        it is difficult to accurately describe exactly what the user wants created. ImageBrush bridges that modality gap
        by instead utilizing exemplar images as instructions an copying the difference between the two. This allows
        for seemingly outstanding results in various task like image fill, generation, and transformation.
    </p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Background</h2>
    <p>There have been several advances in image generation tasks over the last couple of years. 
        Models like CLIP enable efficient and expressive embedding for combining image and text modalities. 
        As a result there has been an explosion image generation models that can use text to guide the image generation process. 
        This work tries to use images instead to inform the generation process. 
        The goal is to use other images as exemplar images that the model uses to either generate a new image or fill in an existing one.</p> 
        
        <p> To achieve this the authors propose a set of novel techniques:
            <ol>
                <li>They introduced a novel image manipulation protocol that can perform numerous operations using an in-context approach along with a metric 
                    to assess the quality of image manipulations.</li>
                    <li>They built a diffusion based framework for image generation along with a hybrid context injection strategy to enable language instructions along with image exemplars.</li>
                    <li>Extensive experimentation proves that the images can succesfully be created from exemplars</li>

            </ol> 
        </p>

<h2> Biography</h2>
    <p>
        <div class="image-container">
            <img src="images/Yasheng Sun.png" alt="Image 1">
            <p> <b>Yasheng Sun: </b> Student of Computer Science, Tokyo Tech University, Has previously worked on graph neural networks, speech recognition and Image Processing</p>
        </div>
        <div class="image-container">
            <img src="images/Yifan Yang.jpg" alt="Image 1">
            <p> <b>Yifan Yang: </b> Researcher at the Microsoft Asia Research Center. Computer vision for video and Efficient Networks</p>
        </div>
        <div class="image-container">
            <img src="images/Houwen Peng.jpg" alt="Image 1">
            <p> <b>Houwen Peng: </b> Senior Researcher at Microsoft Research Asia, works on tiny and efficient deep learning, video object tracking, segmentation and detection, vision transformer, neural architecture search, model compression, vision-language intelligence, saliency detection</p>
        </div>
        <div class="image-container">
            <img src="images/Yifei Shen.jpg" alt="Image 1">
            <p> <b>Yifei Shen: </b> Researcher at Microsoft Research Asia, works on  mean-field asymptotics in machine learning and networking; (2) inductive bias in neural architectures.</p>
        </div>
        <div class="image-container">
            <img src="images/Lili Qiu.png" alt="Image 1">
            <p> <b>Lili Qiu: </b> Assistant Director at Microsoft Research Asia. Works on deep learning for networking</p>
        </div>
        <div class="image-container">
            <img src="images/Han Hu.jpg" alt="Image 1">
            <p> <b>Han Hu: </b> Principal Researcher at Microsoft Research Asia, Worked on the SWIN Transformer, and has made contributions to deformable neural networks
            </p>
        </div>
        <div class="image-container">
            <img src="images/Hideki Koike.jpg" alt="Image 1">
            <p> <b>Hideki Koike: </b> Professor at University of Tokyo</p>
        </div>

    </p>

<h2>Social Impact</h2>
    <p>Just as with many current image generation techniques this archtiecture is prone to potential abuse and the spreading of misinformation. It could potentially be used
    to create deepfakes of trusted public figures or spread misinformation by doctoring images. The quality of the transformation could be used to transform a benign image 
    into the aftermath of an unrelated tragedy, so this technology must be monitored carefully. As image generation capability becomes more detailed and powerful in its 
    generality, we must ensure that this technology is not being used for malicious purposes.
    </p>

<h2> Industry Applications</h2>
    <p>
        We have identified numerous industry applications for this work
        <ul>
            <li>Such a model can have impact in a diverse set of industries</li>
            <li> potential application is in the Visual Effects for Film and television. 
                Digitally editing frames is an expensive, manual and time consuming process. 
                This process can be used to develop workflows where artists can edit certain frames and have the rest of the frames filled in using this method</li>
            <li>This application can be embedded in applications like Adobe Photoshop as a way to allow users to edit images using image modalities</li>
            <li>This can be a stand alone tool where users can provide both image and text prompts to edit images and video.</li>
                
        </ul>
    </p>

<h2>Academic Impact</h2>
    <p>
        This work has the potential to be investigated further in an academic setting as well. The following be considered:
        <ul>
            <li> A better embedding system to create image embeddings that are fed into the U-Net.</li>
            <li>Investigate novel techniques to inject context into a neural network model. </li>
            <li>Investigate methods to handle cases when the prompt and the image are far apart.</li>
            <li>Improve techniques to handle finer details and small objects.</li>

        </ul>
    </p>


    <h2>Summary and Method Breakdown:</h2>
        <p>The authors build upon previous work with image synthesis through CLIP-guided diffusion models and image translation tasks with
            conditional GANs to create a new Exemplar-Based Image Manipulation framework called ImageBrush. To avoid the inherent modality gap that
            comes from text-based image generation the authors instead chose to use image-based instruction using a before and after image to specify a
            transformation to a query image. For such a task the authors devised a generative model that tackles both pairwise visual instruction understanding
            and image synthesis in a unified manner. Necessary to this goal is capturing both the inter-image pair and the intra-image pair relations. Unlike with
            language, relationships in image space are not sequential and contain information dispersed across their spatial dimensions, so the authors utilize a
            <q>Progressive-in-Painting</q> strategy. To establish the intra-correlations among exemplar image transformations and their inter-correlation with the query image,
            they utilize a grid-like image as input. This grid concatenates a manipulation example and a target query. The top two items in this grid from left to right would
            be the exemplar images (E, E&#8217;); the bottom left would be the query image (I); and the last grid item would be random noise (M) that is then filled using a diffusion
            process to produce the transformed query image (I&#8217;). Essentially the authors start with a grid-like image {E, E&#8217;, I, M} and attempt to recover the image {E, E&#8217;, I, I&#8217;}.</p>

            <img src="images/fig2.jpg" alt="Model Architecture" style="width:500px;height:600px;">
        <p>Like other Latent Diffusion approaches the authors utilize cross-attention with a Denoising UNet to condition the input, I, on the pair of exemplar images {E, E&#8217;}.
            The authors noted however that capturing detailed information from instruction images through cross-attention alone is challenging, so the authors also
            utilize self-attention to learn their low-level context using a self-attention mechanism over the grid of inputs. By establishing spatial correspondence
            within image channels, this design enables the ImageBrush model to attentively process features within each channel and effectively capture their interdependencies.</p>

            <img src="images/fig1.jpg" alt="UNet and Cross-Attention Example" style="width:500px;height:600px;">

        <p>So essentially the images are encoded, concatenated, passed through multiple transformer layers for the prompt encoder, and then finally passed through the modified
            UNet to iteratively denoise the image. The authors also utilize a modified conditional predicted noise to accomplish this goal:</p>

        <img src="images/cond_noise.jpg" alt="Conditional Noise Formula">

        <p>This conditioned predicted noise modifies the original predicted noise, epsilon_theta(x_t, null), (noise predicted without attending to
            the exemplars) to more closely match the guidance signal, epsilon_theta(x_t, c), with guidance scale, w, and provided conditions, c.
            While the conditions here are generally the exemplar images, the authors also allow for the use of multimodal
            input such as bounding box user instructions and text descriptions to further improve the diffusion process.</p>
    <h2>Peer-Review</h2>
    <h3> Strengths and Weaknesses: </h3>
        <p>Strengths: Great results that seem to outperform SOTA models in exemplar image generation. Concise and relatively easy-to-understand paper
            The creation of the new prompt and image fidelity evaluation metrics may be important for the evaluation of future models in such a relatively new field.
            Weaknesses: More developed/in-depth discussion about the unique aspects of the model architecture would be desirable.
            There is relatively little discussion of the self-attention aspects of the UNet and specific unique improvements over previous work, though the results do seem strong.
            Evaluation of the model is a little weak. There is some quantitative comparison with Instruct-Pix2Pix, yet a different comparison for TSAM and CoCosNet.
            The authors also utilize a qualitative comparison with pix2pix and SDEdit, rather than either of the other two models they used before.
            While it showed seemingly good results this small qualitative comparison could also be very cherry-picked.
            Created a new evaluation metric but did not use it to compare to any other models
        </p>

    <h3> Questions and Limitations:</h3>
        <p>Addressed some limitations, but a deeper discussion of limitations would be preferred.
            They briefly touched upon how large differences in the exemplar images lead to issues generating a new image and how the model had trouble with fine details.
            Would like further testing and comparison with different models and more discussion of how exemplar similarity affects output results.
            A bit more discussion on how the self-attention was incorporated with the cross-attention would also be nice.
        </p>
    <h3>Scores</h3>
        <p>
            <ul> 
                <li>Soundness: 3 - Could use more evaluation along with a stronger explanation of theoretical principles in certain parts.
            A deeper dive into the limitations of the models would be preferred. More discussion of the model architecture and how the addition of
            self-attention improves the model compared to previous work would be nice.</li>
            <li>Presentation: 4 - Clear and Concise!</li>
            <li>Contribution: 3 - Could be a strong contribution</li>
            Overall: 7 - Accept: Almost wanted to make this a weak accept because of the evaluation but otherwise an interesting paper.
            Could use more discussion of specific architectural improvements, but the results seem promising and it could lead to more
            great research.
            </ul>
        </p>



<h2> Follow-Up Research</h2>
    <p>Models blending text prompting, image-examplars, and user input are a promising step forward in image generation instructions.
        The authors noted issues when there significant disparities between the querya nd instruction and difficulties handling intricate details,
        which would merit some future work to improve. More work in different blends of modalities such as text, user input, and
        images could lead to great breakgthroughs in image generation and transformation.</p>


<h3>References</h3>

<p><a name="ImageBrush">[1]</a> <a href="https://openreview.net/forum?id=EmOIP3t9nk">
    Yasheng Sun et al.;
  <em>ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation;</em></a>
  Thirty-seventh Conference on Neural Information Processing Systems (2023).
</p>

<h2>Team Members</h2>
                                                   
<p>Daniel Varivoda and Tanmay Gadgil</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
